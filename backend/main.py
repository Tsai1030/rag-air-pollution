# -*- coding: utf-8 -*-
from fastapi import FastAPI, Body, Depends, Request
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain.prompts import PromptTemplate
from datetime import datetime
import logging, time, json, os, torch
from pathlib import Path
from typing import Optional, List
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from functools import lru_cache
import random
import re # <--- éœ€è¦ re æ¨¡çµ„
# import time # time å·²ç¶“åœ¨ä¸Šé¢ import éäº†

# âœ… FastAPI åˆå§‹åŒ–
app = FastAPI()


# âœ… åˆ¤æ–·ä½¿ç”¨è€…æ˜¯å¦æœ‰æŒ‡å®šæ ¼å¼éœ€æ±‚
def detect_format_mode(question: str) -> str:
    format_triggers = [
        "è«‹ç”¨ä¸€æ®µè©±", "æ‘˜è¦", "è¡¨æ ¼", "è¡¨åˆ—", "æ¢åˆ—å¼", "æ¸…å–®å½¢å¼", "ä¸€å¥è©±", "èªªæ˜å°±å¥½",
        "summarize", "as a table", "one paragraph", "bullet points", "list format"
    ]
    # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼ä¾†æ›´ç²¾ç¢ºåœ°åŒ¹é…ï¼Œé¿å…éƒ¨åˆ†åŒ¹é… (ä¾‹å¦‚ "æ¢åˆ—" ä¸æœƒåŒ¹é…åˆ° "ç„¡æ¢ç†")
    # ä¸¦ä¸”å¿½ç•¥å¤§å°å¯«
    if any(re.search(r'\b' + re.escape(kw) + r'\b', question, re.IGNORECASE) for kw in format_triggers) or "ç°¡å–®èªªæ˜" in question:
         return "custom"
    return "default"

# --- Middleware Setup ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­æ‡‰æ›´åš´æ ¼
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(
    TrustedHostMiddleware,
    allowed_hosts=["*"] # åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­æ‡‰æŒ‡å®šå…è¨±çš„ä¸»æ©Ÿå
)

# --- Logging Configuration ---
# é…ç½®æ—¥èªŒè¨˜éŒ„å™¨ï¼Œå¢åŠ  DEBUG ç´šåˆ¥é¸é …
log_level = os.environ.get("LOG_LEVEL", "INFO").upper()
logging.basicConfig(level=log_level, format="%(asctime)s - %(levelname)s - %(message)s")
logging.info(f"æ—¥èªŒç´šåˆ¥è¨­å®šç‚º: {log_level}")


# --- Global Variables & Constants ---
MAX_SESSIONS = 1000
MAX_HISTORY_PER_SESSION = 10
device = "cuda" if torch.cuda.is_available() else "cpu"
logging.info(f"ä½¿ç”¨è¨­å‚™: {device}")

embedding = None
vectordb = None
available_models = {}
chat_memory = {} # ä½¿ç”¨ LRU Cache å¯èƒ½æ›´é©åˆç®¡ç†å…§å­˜ï¼Œä½†ç°¡å–®å­—å…¸ä¹Ÿå¯ä»¥
request_counters = {} # ç”¨æ–¼é€Ÿç‡é™åˆ¶

FEEDBACK_SAVE_PATH = Path("manual_feedback")
QA_LOG_PATH = Path("qa_logs")
SAVE_QA = True
MAX_LLM_RETRIES = 1 # LLM å‘¼å«é‡è©¦æ¬¡æ•¸

# --- Embedding Model Loading ---
try:
    # è€ƒæ…®å°‡æ¨¡å‹åç¨±è¨­ç‚ºç’°å¢ƒè®Šæ•¸
    embedding_model_name = os.environ.get("EMBEDDING_MODEL", "BAAI/bge-m3")
    logging.info(f"æ­£åœ¨è¼‰å…¥åµŒå…¥æ¨¡å‹: {embedding_model_name}")
    embedding = HuggingFaceEmbeddings(
        model_name=embedding_model_name,
        model_kwargs={"device": device},
        encode_kwargs={"normalize_embeddings": True} # bge-m3 å»ºè­°è¨­ç‚º True
    )
    # æ¸¬è©¦åµŒå…¥æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
    _ = embedding.embed_query("æ¸¬è©¦åµŒå…¥æ¨¡å‹")
    logging.info("âœ… åµŒå…¥æ¨¡å‹è¼‰å…¥ä¸¦æ¸¬è©¦æˆåŠŸ")
except Exception as e:
    logging.error(f"âŒ åµŒå…¥æ¨¡å‹è¼‰å…¥å¤±æ•—: {str(e)}", exc_info=True)
    embedding = None # ç¢ºä¿å¤±æ•—æ™‚ç‚º None

# --- Vector Database Loading (on Startup) ---
@app.on_event("startup")
def load_vector_database():
    global vectordb
    if embedding is None:
        logging.error("âŒ åµŒå…¥æ¨¡å‹æœªè¼‰å…¥ï¼Œç„¡æ³•åˆå§‹åŒ–å‘é‡è³‡æ–™åº«ã€‚")
        return
    try:
        # è€ƒæ…®å°‡è³‡æ–™åº«è·¯å¾‘è¨­ç‚ºç’°å¢ƒè®Šæ•¸
        persist_dir = os.environ.get("VECTORDB_PATH", "5_5test")
        logging.info(f"æ­£åœ¨å¾ '{persist_dir}' è¼‰å…¥å‘é‡è³‡æ–™åº«...")
        if not os.path.exists(persist_dir):
            logging.error(f"âŒ å‘é‡è³‡æ–™åº«ç›®éŒ„ '{persist_dir}' ä¸å­˜åœ¨ï¼Œè«‹å»ºç«‹æˆ–æª¢æŸ¥è·¯å¾‘ã€‚")
            return
        vectordb = Chroma(persist_directory=persist_dir, embedding_function=embedding)
        # å˜—è©¦é€²è¡Œä¸€æ¬¡æŸ¥è©¢ä»¥é ç†±ä¸¦é©—è­‰
        _ = vectordb.similarity_search("ç³»çµ±é ç†±", k=1)
        logging.info(f"âœ… å‘é‡è³‡æ–™åº«å¾ '{persist_dir}' è¼‰å…¥ä¸¦é ç†±å®Œæˆ")
    except Exception as e:
        logging.error(f"âŒ å‘é‡è³‡æ–™åº«è¼‰å…¥å¤±æ•—: {str(e)}", exc_info=True)
        vectordb = None # ç¢ºä¿å¤±æ•—æ™‚ç‚º None

# --- Prompt Templates ---
# (ä¿æŒä½ æä¾›çš„ Prompt æ¨¡æ¿ä¸è®Šï¼Œé€™è£¡çœç•¥ä»¥ç¯€çœç©ºé–“)
# --- é¢¨æ ¼ 1ï¼šçµæ§‹åŒ–åˆ—è¡¨ ---
STRUCTURED_LIST_PROMPT = PromptTemplate(
    input_variables=["question", "context", "history", "format_mode"],
    template="""
You are a helpful assistant providing clear and structured information in **Traditional Chinese**. Your task is to answer the user's question based on the retrieved context below, adhering strictly to the specified format based on the `format_mode`.

ğŸ“Œ **Format Mode:** {format_mode}

ğŸš¨ **æ ¼å¼æŒ‡ä»¤ (Format Instructions - åš´æ ¼éµå®ˆ):**

*   **If `format_mode` is `custom`:** ä»£è¡¨ä½¿ç”¨è€…åœ¨å•é¡Œä¸­æŒ‡å®šäº†æ ¼å¼ã€‚è«‹ **å®Œå…¨éµå¾ªä½¿ç”¨è€…è¦æ±‚çš„æ ¼å¼** é€²è¡Œå›ç­”ã€‚å¦‚æœä½¿ç”¨è€…è¦æ±‚éš±å«äº† Markdown çµæ§‹ (å¦‚åˆ—è¡¨)ï¼Œè«‹ä½¿ç”¨æ¨™æº– Markdown (`* `, `1. `)ã€‚ç¢ºä¿ `**ç²—é«”**` ä½¿ç”¨å…©å€‹æ˜Ÿè™Ÿã€‚
*   **If `format_mode` is `default`:** ä»£è¡¨ç³»çµ±éš¨æ©Ÿé¸ä¸­äº†ä¸‹è¿°çš„ã€Œçµæ§‹åŒ–åˆ—è¡¨é¢¨æ ¼ã€ã€‚ä½  **å¿…é ˆ** åš´æ ¼ä½¿ç”¨æ­¤é¢¨æ ¼ï¼Œä»»ä½•åå·®éƒ½è¦–ç‚ºéŒ¯èª¤ã€‚
    *   **æœ€é—œéµè¦æ±‚ (CRITICAL for default mode):** ç« ç¯€æ¨™é¡Œ **å¿…é ˆ** ä½¿ç”¨ **å…©å€‹æ˜Ÿè™Ÿ** åŒ…è£¹ (æ ¼å¼ç‚º `**ä¸€ã€ä¸­æ–‡æ¨™é¡Œ**`)ã€‚**çµ•å°ç¦æ­¢** ä½¿ç”¨å¤šé¤˜çš„æ˜Ÿè™Ÿ (å¦‚ `***æ¨™é¡Œ***` æˆ– `* **æ¨™é¡Œ** *`) æˆ–å–®å€‹æ˜Ÿè™Ÿã€‚**æ¨™é¡Œå¾Œçš„å…§å®¹æ–‡å­—çµ•ä¸èƒ½åŠ ç²—ã€‚**

ğŸ¯ **é è¨­å›æ‡‰æ ¼å¼ (Default Response Format - çµæ§‹åŒ–åˆ—è¡¨é¢¨æ ¼ - åƒ…åœ¨ `format_mode` ç‚º 'default' æ™‚ä½¿ç”¨):**

1.  **å¼•è¨€ (Introduction):** åœ¨é–‹é ­æä¾›ä¸€å€‹ç°¡çŸ­çš„å¼•è¨€ (1-2 å¥è©±)ã€‚
2.  **ç·¨è™Ÿç« ç¯€ (Numbered Sections):** åŒ…å« 2 è‡³ 5 å€‹ç« ç¯€ï¼Œä½¿ç”¨ `**ä¸€ã€**`, `**äºŒã€**` ç­‰ä½œç‚ºæ¨™é¡Œå‰ç¶´ï¼Œæ¨™é¡Œæœ¬èº«éœ€åŠ ç²—ï¼Œå®Œæ•´æ ¼å¼ç‚º `**ä¸€ã€ä¸­æ–‡æ¨™é¡Œ**`ã€‚
3.  **ç« ç¯€å…§å®¹ (Content):** æ¯å€‹ç« ç¯€æ¨™é¡Œä¸‹æ–¹æ’°å¯« 1-3 å¥è©±çš„å…§å®¹ã€‚**å…§å®¹å¿…é ˆæ˜¯ç´”æ–‡å­— (plain text)**ï¼Œä¸å¾—åŠ ç²—æˆ–ä½¿ç”¨æ–œé«”ã€‚èªè¨€éœ€æ¸…æ™°æ˜“æ‡‚ã€‚å¯ä»¥åœ¨å…§å®¹æ–‡å­—ä¸­ **å°‘é‡** ä½¿ç”¨ç›¸é—œçš„è¡¨æƒ…ç¬¦è™Ÿ (è¦‹ä¸‹æ–¹å»ºè­°)ï¼Œä½† **æ¨™é¡Œä¸­ç¦æ­¢ä½¿ç”¨ä»»ä½•è¡¨æƒ…ç¬¦è™Ÿ**ã€‚
4.  **åˆ†éš”ç·š (Separator Line):** åœ¨ **æ¯å€‹** ç« ç¯€çš„å…§å®¹æ–‡å­—çµæŸå¾Œï¼Œ**å¿…é ˆ** æ’å…¥ä¸€è¡Œç”± **100å€‹** åŠå½¢å¥é» (`.`) çµ„æˆçš„åˆ†éš”ç·šï¼Œå¦‚ä¸‹ï¼š
    ....................................................................................................
5.  **é–“è· (Spacing):** å¼•è¨€å’Œç¬¬ä¸€å€‹ç« ç¯€æ¨™é¡Œä¹‹é–“ä¿ç•™ä¸€å€‹ç©ºè¡Œã€‚æ¯å€‹åˆ†éš”ç·šå’Œä¸‹ä¸€å€‹ç« ç¯€æ¨™é¡Œä¹‹é–“ä¹Ÿä¿ç•™ä¸€å€‹ç©ºè¡Œã€‚
6.  **èªè¨€ (Language):** å…¨æ–‡ä½¿ç”¨ **ç¹é«”ä¸­æ–‡**ã€‚
7.  **å…§å®¹ä¾†æº (Context Usage):** å›ç­”å…§å®¹ **åƒ…èƒ½** æ ¹æ“šä¸‹æ–¹æä¾›çš„ `Retrieved Context` ç”Ÿæˆï¼Œä¸è¦æåŠã€Œæ ¹æ“šä¸Šä¸‹æ–‡ã€æˆ–ç›´æ¥è¤‡è£½ä¸Šä¸‹æ–‡åŸæ–‡ã€‚
8.  **è¡¨æƒ…ç¬¦è™Ÿå»ºè­° (Emojis - åƒ…ç”¨æ–¼å…§å®¹æ–‡å­—):** ğŸ’¡, âœ…, âš ï¸, ğŸ“Š, ğŸ‘¥, ğŸ«, ğŸŒ±, ğŸ¤, â¤ï¸ (æˆ–å…¶ä»–èˆ‡å…§å®¹ç›¸é—œçš„ç¬¦è™Ÿ)

ğŸ“˜ **Conversation History:** {history}
ğŸ“„ **Retrieved Context:** {context}
â“ **User Question:** {question}

ğŸ‘‡ **è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚æ ¹æ“šåµæ¸¬åˆ°çš„ `format_mode` éµå¾ªå°æ‡‰çš„æ ¼å¼æŒ‡ä»¤ã€‚è‹¥ç‚º `default` æ¨¡å¼ï¼Œè«‹æ¥µåº¦åš´æ ¼åœ°éµå®ˆã€Œçµæ§‹åŒ–åˆ—è¡¨é¢¨æ ¼ã€çš„æ‰€æœ‰ç´°ç¯€ï¼Œç‰¹åˆ¥æ˜¯ `**æ¨™é¡Œ**` æ ¼å¼å’Œç´”æ–‡å­—å…§å®¹è¦æ±‚ã€‚**

ğŸ“ **è¼¸å‡ºç¯„ä¾‹ (EXAMPLE OUTPUT FORMAT - when `format_mode` is "default"):**

å°æ¸¯ç©ºæ±¡USRè¨ˆç•«å·²å±•ç¾å‡ºä¸€å®šçš„æˆæ•ˆ âœ…ï¼Œä¸»è¦é«”ç¾åœ¨æå‡ç¤¾å€å±…æ°‘å°ç©ºæ±¡è­°é¡Œçš„èªçŸ¥ã€ä¿ƒé€²å¥åº·è¡Œç‚ºçš„æ”¹è®Šä»¥åŠå»ºç«‹ç¤¾å€åƒèˆ‡çš„æ©Ÿåˆ¶ã€‚

**ä¸€ã€ç¤¾å€åƒèˆ‡èˆ‡æ•™è‚²æ¨å»£**
è¨ˆç•«åœ˜éšŠæ·±å…¥å°æ¸¯å€çš„å­¸æ ¡ ğŸ« èˆ‡ç¤¾å€ ğŸ‘¥ï¼Œèˆ‰è¾¦å„é¡ç’°å¢ƒèˆ‡å¥åº·æ•™è‚²æ´»å‹•ï¼Œä¾‹å¦‚é‡å°ç©ºæ±¡æ•æ„Ÿæ—ç¾¤çš„å…’ç«¥æ°£å–˜è¡›æ•™ç‡ŸéšŠï¼Œä»¥åŠåœ¨é³³æ—åœ‹å°èˆ‰è¾¦çš„ã€Œç©ºæ±¡å¥åº·ç«™ã€ç’°å¢ƒæ•™è‚²å˜‰å¹´è¯ã€‚é€™äº›æ´»å‹•å¸å¼•äº†æ•¸ç™¾åå°æ¸¯åœ°å€å±…æ°‘åƒèˆ‡ï¼ŒæˆåŠŸå°‡ç©ºæ±¡çŸ¥è­˜è½‰åŒ–ç‚ºç¤¾å€è¡Œå‹•ã€‚
....................................................................................................

**äºŒã€é‡å°ç‰¹å®šæ—ç¾¤çš„å¥åº·ä¿ƒé€²**
è¨ˆç•«é‡å°ç©ºæ±¡æ•æ„Ÿæ—ç¾¤çš„å…’ç«¥æ°£å–˜é€²è¡Œè¡›æ•™ï¼Œä¸¦æ“´åŠé«˜é½¡æ—ç¾¤ï¼Œä¾‹å¦‚èˆ‰è¾¦é«˜é½¡è€…ç¤¾å€å¥åº·ä¿ƒé€²è¬›åº§å’Œé•·ç…§æ“šé»åˆä½œçš„å‘¼å¸ä¿å¥èª²ç¨‹ã€‚é€™äº›æ´»å‹•æå‡äº†ä¸åŒå¹´é½¡å±¤å°ç©ºæ°£å“è³ªèˆ‡å¥åº·é¢¨éšªçš„èªçŸ¥ï¼Œä¸¦ä¿ƒé€²äº†å¥åº·è¡Œç‚ºçš„æ”¹è®Š â¤ï¸ã€‚
....................................................................................................

**ä¸‰ã€èˆ‡ä¼æ¥­åˆä½œçš„äº®é»**
è¨ˆç•«èˆ‡å°æ¸¯é†«é™¢åŠåœ°æ–¹ä¼æ¥­åˆä½œ ğŸ¤ æ¨å‹•ç©ºæ±¡ç›£æ¸¬ç³»çµ±å’ŒESGå¥åº·ä¿ƒé€²æ–¹æ¡ˆï¼Œé¡¯ç¤ºè¨ˆç•«åœ¨æ•´åˆè³‡æºã€ä¿ƒé€²ç¤¾å€ç™¼å±•æ–¹é¢çš„åŠªåŠ›ã€‚
....................................................................................................

**å››ã€è¨ˆç•«ç›®æ¨™çš„è½å¯¦èˆ‡æˆæœåˆ†äº«**
è¨ˆç•«ç”³è«‹æ›¸å®Œæ•´é—¡è¿°äº†å°æ¸¯ç©ºæ±¡è­°é¡Œçš„èƒŒæ™¯ã€ç›®æ¨™ã€åŸ·è¡Œæ–¹æ¡ˆèˆ‡é æœŸæ•ˆç›Š ğŸ“Šï¼Œä¸¦å®šæœŸæäº¤é€²åº¦èˆ‡æˆæœå ±å‘Šï¼Œç¢ºä¿è¨ˆç•«ç›®æ¨™çš„è½å¯¦ï¼Œä¸¦é€šéå¤šå…ƒæ–¹å¼é€²è¡Œæˆæ•ˆè©•ä¼°èˆ‡åˆ†äº«ã€‚
....................................................................................................
"""
)
# --- é¢¨æ ¼ 2ï¼šéšå±¤å¼æ¢åˆ— ---
# --- Style 2: Hierarchical Lists (English Instructions, Chinese Output) ---
# --- Style 2: Hierarchical Lists (English Instructions, Chinese Output - Revised) ---
HIERARCHICAL_BULLETS_PROMPT = PromptTemplate(
    input_variables=["question", "context", "history", "format_mode"],
    template="""
You are a helpful assistant tasked with providing detailed, hierarchically structured information **in Traditional Chinese**. Your task is to answer the user's question based on the retrieved context below using **standard Markdown headings and lists** for structure. This style was randomly selected for the default mode.

ğŸ“Œ **Format Mode:** {format_mode} (System detected: 'default' - no specific user request in question)

ğŸš¨ **Format Instructions (Strictly Adhere to this Style):**

*   Your response **MUST** be entirely in **Traditional Chinese**, structured using standard Markdown.
*   **Headings:**
    *   Use `# ` (one hash + space) for the optional **Main Title**.
    *   Use `## ` (two hashes + space) for **Main Section Headers** (e.g., `## ä¸€ã€Main Section Title`).
    *   Use `### ` (three hashes + space) for **Sub-section Headers** (e.g., `### 1. Sub-section Title`).
*   **Lists:**
    *   Below headings, when itemization is needed, use standard Markdown **unordered lists** (`* ` or `- ` followed by a space) or **ordered lists** (`1. `, `2. ` followed by a space).
    *   List item text must be **plain text starting immediately after the marker and space**. **CRITICAL: Absolutely DO NOT apply bold markdown (`**`) to the text immediately following the list marker (`* / - / 1. `) or any introductory label ending with a colon (like `Label:`).** Use `**bold emphasis**` very sparingly *only* for specific keywords deep *within* the list item text itself.
    *   Maintain consistent list indentation (typically 2 or 4 spaces).
*   **Spacing:** Ensure one blank line between the Main Title (if used) and the first Section Header, and one blank line between subsequent `##` Section Headers.
*   **Critical Requirements:**
    *   **Strictly adhere** to the Markdown syntax specified above, especially the **required spaces** after heading markers (`#`, `##`, `###`) and list markers (`*`, `-`, `1.`).
    *   **Absolutely prohibit** non-standard formats or extra symbols.
    *   **Bold (`**`)** is used *only* for emphasizing specific words within paragraphs or list items. **Do not bold entire headings or list items, and especially not the text immediately following a list marker or label.**
*   **Context Usage:** Base your answer *only* on the provided `Retrieved Context`. **Absolutely prohibit mentioning** "According to the text," "The text indicates," "The text doesn't mention," or any phrases referencing the context source. State the information derived from the context directly.
*   **No Preamble / Meta-commentary:** Start the response **directly** with the required formatted content (e.g., the `#` title or first `##` heading). **Do not** add introductory phrases like "Okay, here is the information..." or comments about the format itself like "Here is the response in hierarchical format:".
*   **No Conversational Closing:** Do not add concluding remarks like "Hope this helps!" or follow-up questions.

ğŸ“˜ **Conversation History:**
{history}

ğŸ“„ **Retrieved Context:**
{context}

â“ **User Question:**
{question}

ğŸ‘‡ **Respond entirely in Traditional Chinese.** Strictly follow the 'Hierarchical Markdown' format: use headings with spaces (`# /## /### `) and standard lists (`* / - / 1. `). **Ensure list item text immediately following the marker or a label ending in a colon is plain text, NOT bold.** Start directly with the formatted content.

ğŸ“ **Example Output Format (when `format_mode` is "default" and this style is chosen):**

# ç©ºæ°£æ±¡æŸ“æ•™è‚²æˆæ•ˆè©•ä¼°æ–¹æ³•

åœ¨å°ç£çš„å¤§å­¸ç¤¾æœƒè²¬ä»»ï¼ˆUSRï¼‰è¨ˆç•«ä¸­ï¼Œè©•ä¼°ã€Œç©ºæ°£æ±¡æŸ“æ•™è‚²æˆæ•ˆã€æ˜¯ä¸€é …å¤šé¢å‘çš„ä»»å‹™ï¼Œæ‡‰çµåˆé‡åŒ–èˆ‡è³ªåŒ–æŒ‡æ¨™ï¼Œä»¥å…¨é¢äº†è§£æ•™è‚²æ´»å‹•å°ç¤¾å€ã€å­¸ç”Ÿèˆ‡æ”¿ç­–å±¤é¢çš„å½±éŸ¿ã€‚ä»¥ä¸‹æ˜¯å¹¾ç¨®å¸¸è¦‹ä¸”å»ºè­°ä½¿ç”¨çš„è©•ä¼°æ–¹å¼ï¼š

## ä¸€ã€é‡åŒ–æŒ‡æ¨™ï¼ˆQuantitative Evaluationï¼‰

### 1. å‰å¾Œæ¸¬å•å·åˆ†æ
*   é‡å°åƒèˆ‡è€…ï¼ˆå¦‚å­¸ç”Ÿã€ç¤¾å€å±…æ°‘ï¼‰åœ¨èª²ç¨‹æˆ–æ´»å‹•å‰å¾Œé€²è¡ŒçŸ¥è­˜ã€æ…‹åº¦èˆ‡è¡Œç‚ºæ„å‘æ¸¬é©—ã€‚
*   æ¯”è¼ƒå…¶ç’°å¢ƒçŸ¥è­˜å¢é•·ã€å°ç©ºæ±¡è­°é¡Œçš„**æ•æ„Ÿåº¦**æå‡ã€‚

### 2. åƒèˆ‡äººæ•¸èˆ‡æ´»å‹•å ´æ¬¡
*   çµ±è¨ˆå¯¦é«”æˆ–ç·šä¸Šèª²ç¨‹ã€è¬›åº§ã€å·¥ä½œåŠåƒèˆ‡äººæ¬¡ã€‚
*   é•·æœŸè¿½è¹¤æ˜¯å¦æœ‰å›ºå®šåƒèˆ‡æ—ç¾¤ï¼Œæˆ–æ˜¯å¦èƒ½è§¸åŠæ–°å°è±¡ã€‚

### 3. è¡Œç‚ºæ”¹è®Šçš„æŒ‡æ¨™
*   å¦‚å±…æ°‘æ˜¯å¦é–‹å§‹ä½¿ç”¨ç©ºæ°£å“è³ªç›£æ¸¬å™¨ã€æ”¹è®Šäº¤é€šå·¥å…·ä½¿ç”¨ç¿’æ…£ã€‚
*   å­¸æ ¡æ˜¯å¦æ¨å‹•æ ¡åœ’ç¶ åŒ–ç­‰ã€‚

### 4. ç¤¾ç¾¤åª’é«”èˆ‡å¹³å°äº’å‹•
*   è§€çœ‹æ¬¡æ•¸ã€åˆ†äº«ã€ç•™è¨€ã€é»è®šæ•¸ç­‰å¯è¡¡é‡è³‡è¨Šæ“´æ•£æˆæ•ˆã€‚

## äºŒã€è³ªåŒ–æŒ‡æ¨™ï¼ˆQualitative Evaluationï¼‰

### 1. æ·±åº¦è¨ªè«‡èˆ‡ç„¦é»åœ˜é«”
*   é€éèˆ‡å­¸ç”Ÿã€æ•™å¸«ã€å±…æ°‘åŠåœ¨åœ° NGO è¨ªè«‡ï¼Œç­è§£ç©ºæ±¡æ•™è‚²å¸¶ä¾†çš„è§€å¿µæ”¹è®Šæˆ–ç”Ÿæ´»å¯¦è¸ã€‚
*   ç­è§£åƒèˆ‡è€…å°æ•™è‚²å…§å®¹çš„æ¥å—åº¦èˆ‡å»ºè­°ã€‚

### 2. å­¸ç”Ÿèˆ‡å±…æ°‘çš„åæ€ç´€éŒ„æˆ–å­¸ç¿’æˆæœ
*   åŒ…å«å­¸ç¿’å–®ã€åæ€æ—¥èªŒã€å‰µä½œï¼ˆå¦‚çŸ­ç‰‡ã€æµ·å ±ï¼‰ç­‰ã€‚
*   ä½œç‚ºå…¶å…§åŒ–æˆæœçš„å‘ˆç¾ã€‚

### 3. ç¤¾å€åˆä½œçš„å¯¦è³ªæˆæœ
*   å¦‚èˆ‡åœ°æ–¹æ”¿åºœã€å­¸æ ¡æˆ–ç¤¾å€åˆä½œå»ºç«‹ç©ºæ±¡ç›£æ¸¬ç«™ã€‚
*   å…±åŒæå‡ºæ”¹å–„å»ºè­°ç­‰ã€‚

## ä¸‰ã€é•·æœŸæˆæ•ˆè¿½è¹¤ï¼ˆImpact Trackingï¼‰

### 1. æ”¿ç­–å½±éŸ¿åŠ›
*   æ˜¯å¦ä¿ƒæˆåœ°æ–¹æ”¿åºœæˆ–å­¸æ ¡åœ¨ç©ºæ±¡è­°é¡Œä¸Šçš„æ”¿ç­–ä¿®è¨‚ã€‚
*   æ¨å‹•å¯¦ä½œæ–¹æ¡ˆã€‚

### 2. ç¤¾å€æ„è­˜æŠ¬é ­èˆ‡è‡ªç™¼è¡Œå‹•
*   æ˜¯å¦å‡ºç¾è‡ªä¸»è¾¦ç†ç›¸é—œæ´»å‹•ã€‚
*   æˆç«‹è‡ªæ•‘æœƒæˆ–ç›£ç£å¹³å°ã€‚

### 3. è·¨é ˜åŸŸèˆ‡æ°¸çºŒæ“´æ•£
*   è©•ä¼°æ˜¯å¦èƒ½èˆ‡å…¶ä»– USR åœ˜éšŠã€ç ”ç©¶å–®ä½æˆ–ä¼æ¥­å½¢æˆåˆä½œã€‚
*   å°‡æ•™è‚²æ¨¡å¼æ“´å±•è‡³å…¶ä»–å€åŸŸæˆ–ä¸»é¡Œã€‚

*(Note: The example output deliberately excludes concluding summaries or questions.)*
"""
)
# --- é¢¨æ ¼ 3ï¼šæ®µè½å‰ç½®åœ–æ¨™ ---
PARAGRAPH_EMOJI_LEAD_PROMPT = PromptTemplate(
    input_variables=["question", "context", "history", "format_mode"],
    template="""
You are a helpful assistant providing clear, paragraph-based explanations in **Traditional Chinese**. Your task is to answer the user's question based on the retrieved context below, using a specific style where each paragraph starts with a relevant emoji. This style was randomly selected for the default mode.

ğŸ“Œ **Format Mode:** {format_mode} (System detected: 'default' - no specific user request in question)

ğŸš¨ **æ ¼å¼æŒ‡ä»¤ (åš´æ ¼éµå®ˆæ­¤é¢¨æ ¼):**

*   ä½ çš„å›ç­” **å¿…é ˆ** æ˜¯ä¸€ç³»åˆ— **ç¹é«”ä¸­æ–‡** çš„æ®µè½ã€‚
*   **æœ€é—œéµçš„æ˜¯ï¼šæ¯å€‹æ®µè½éƒ½å¿…é ˆä»¥ `ä¸€å€‹ç›¸é—œçš„è¡¨æƒ…ç¬¦è™Ÿ` + `ä¸€å€‹ç©ºæ ¼` é–‹é ­ã€‚** è¡¨æƒ…ç¬¦è™Ÿæ‡‰èˆ‡è©²æ®µè½çš„ä¸»é¡Œç›¸é—œã€‚
*   **è¡¨æƒ…ç¬¦è™Ÿå’Œç©ºæ ¼ä¹‹å¾Œçš„æ–‡å­—å…§å®¹ï¼Œå¿…é ˆæ˜¯ç´”æ–‡å­— (plain text)ã€‚** çµ•å°ä¸è¦è‡ªå‹•å°‡é€™éƒ¨åˆ†æ–‡å­—åŠ ç²—ã€‚
*   **å®Œå…¨é¿å…** åœ¨å›ç­”ä¸­ä½¿ç”¨ä»»ä½•ç·¨è™Ÿåˆ—è¡¨ (`1.`, `2.`)ã€é …ç›®ç¬¦è™Ÿåˆ—è¡¨ (`*`, `-`)ã€ç« ç¯€æ¨™é¡Œ (`#`, `##`, `**æ¨™é¡Œ**`) æˆ–åˆ†éš”ç·š (`---`, `...`)ã€‚å°ˆæ³¨æ–¼ç´”æ®µè½çµæ§‹ã€‚
*   å¦‚æœéœ€è¦åœ¨æ®µè½ *å…§éƒ¨* å¼·èª¿ç‰¹å®šé—œéµå­—ï¼Œå¯ä»¥ **éå¸¸å°‘é‡åœ°** ä½¿ç”¨æ¨™æº– Markdown ç²—é«” (`**å¼·èª¿è©**`)ã€‚**åˆ‡å‹¿** å°‡è¡¨æƒ…ç¬¦è™Ÿå¾Œçš„ç¬¬ä¸€å€‹å®Œæ•´å¥å­åŠ ç²—ã€‚

ğŸ’¡ **è¡¨æƒ…ç¬¦è™Ÿå»ºè­° (æ ¹æ“šæ®µè½ä¸»é¡Œé¸æ“‡ï¼Œä¹Ÿå¯ä½¿ç”¨å…¶ä»–ç›¸é—œç¬¦è™Ÿ):**
    ğŸ’¡, ğŸ¤, ğŸ¥, ğŸ‘¥, ğŸ“š, ğŸŒ±, ğŸ”¬, ğŸ§­, âœ…, âš ï¸, ğŸ“Š, ğŸ«

ğŸ“˜ **Conversation History:** {history}
ğŸ“„ **Retrieved Context:** {context}
â“ **User Question:** {question}

ğŸ‘‡ **è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚åš´æ ¼éµå¾ªã€Œæ®µè½å‰ç½®è¡¨æƒ…ç¬¦è™Ÿã€æ ¼å¼ï¼šæ¯å€‹æ®µè½ä»¥å–®ä¸€è¡¨æƒ…ç¬¦è™Ÿ+ç©ºæ ¼é–‹é ­ï¼Œå¾Œé¢æ¥çºŒç´”æ–‡å­—ã€‚**

ğŸ“ **è¼¸å‡ºç¯„ä¾‹ (ç•¶ `format_mode` ç‚º "default" ä¸”é¸ä¸­æ­¤é¢¨æ ¼æ™‚):**

ğŸ’¡ åœ¨USRï¼ˆå¤§å­¸ç¤¾æœƒè²¬ä»»ï¼‰è¨ˆç•«ä¸­ï¼Œèˆ‡é†«ç™‚æ©Ÿæ§‹çš„åˆä½œæ˜¯æ¨å‹•ç¤¾å€å¥åº·ä¿ƒé€²çš„é‡è¦ä¸€ç’°ã€‚è©²è¨ˆç•«é€šéå¤šç¨®æ–¹å¼ä¾†ç¢ºä¿å±…æ°‘èƒ½å¤ ç²å¾—æŒçºŒä¸”æœ‰æ•ˆçš„å¥åº·ç®¡ç†æœå‹™ã€‚

ğŸ¥ é¦–å…ˆï¼ŒUSRè¨ˆç•«èˆ‡åœ°æ–¹æ”¿åºœåŠé†«ç™‚å–®ä½åˆä½œè¨­ç«‹å¥åº·æª¢æŸ¥ç«™ï¼Œä¸¦å®šæœŸé€²è¡Œç›£æ¸¬ã€‚é€™äº›å¥åº·æª¢æŸ¥ç«™ä¸åƒ…æä¾›åŸºæœ¬çš„é«”æª¢æœå‹™ï¼Œé‚„åŒ…æ‹¬é‡å°ç©ºæ°£æ±¡æŸ“ç­‰ç’°å¢ƒå› ç´ å°å¥åº·å½±éŸ¿çš„å°ˆé–€è©•ä¼°ã€‚

ğŸ‘¥ æ­¤å¤–ï¼ŒUSRè¨ˆç•«ä¹Ÿçµ„ç¹”è¯åˆè¡›æ•™æ´»å‹•å’Œç¤¾å€å¯¦è¸é …ç›®ï¼Œä¿ƒä½¿æ ¡åœ’èˆ‡ç¤¾å€å½¢æˆç·Šå¯†äº’å‹•èˆ‡å”åŒç™¼å±•ã€‚é€™äº›æ´»å‹•æ—¨åœ¨å…¨æ–¹ä½æå‡å±…æ°‘çš„ç”Ÿæ´»å“è³ªå’Œå¥åº·æ°´å¹³ã€‚

ğŸ¤ ç‚ºäº†é€²ä¸€æ­¥æ¨å‹•ç¤¾å€å¥åº·ä¿ƒé€²ï¼ŒUSRè¨ˆç•«é‚„å¼·èª¿äº†èˆ‡é†«ç™‚æ©Ÿæ§‹åœ¨è³‡æºæ•´åˆä¸Šçš„é‡è¦æ€§ã€‚é€™åŒ…æ‹¬åˆ©ç”¨å­¸è¡“ç ”ç©¶çš„åŠ›é‡ä¾†é–‹ç™¼æ–°çš„å¥åº·ç”¢å“å’Œæœå‹™ã€‚

âœ… ç¸½ä¹‹ï¼ŒUSRè¨ˆç•«é€éèˆ‡é†«ç™‚æ©Ÿæ§‹çš„åˆä½œï¼Œå¾å¤šå€‹å±¤é¢æ¨å‹•ç¤¾å€å¥åº·ä¿ƒé€²å·¥ä½œï¼Œç¢ºä¿äº†å±…æ°‘èƒ½å¤ ç²å¾—å…¨é¢ä¸”æœ‰æ•ˆçš„å¥åº·ç®¡ç†æœå‹™ã€‚
"""
)
# --- æ¨¡æ¿ï¼šè™•ç†ä½¿ç”¨è€…åœ¨å•é¡Œä¸­æŒ‡å®šæ ¼å¼çš„æƒ…æ³ ---
CUSTOM_FORMAT_BASE_PROMPT = PromptTemplate(
    input_variables=["question", "context", "history", "format_mode"],
    template="""
You are a helpful assistant providing information in **Traditional Chinese**. The user has asked a question and appears to have included specific instructions on the desired response format within their question text.

ğŸ“Œ **Format Mode:** {format_mode} (System detected: 'custom' - likely user-specified format in question)

ğŸš¨ **æœ€å„ªå…ˆæŒ‡ä»¤ (ABSOLUTE TOP PRIORITY):**
ä»”ç´°åˆ†æä¸‹æ–¹çš„ **User Question**ã€‚ä½ çš„ **é¦–è¦ä»»å‹™** æ˜¯ **ç²¾ç¢ºåœ°ç†è§£ä¸¦åš´æ ¼éµå®ˆ** ä½¿ç”¨è€…åœ¨å•é¡Œæ–‡å­—ä¸­åŒ…å«çš„ **ä»»ä½•é—œæ–¼è¼¸å‡ºæ ¼å¼çš„æ˜ç¢ºæŒ‡ç¤º**ã€‚
*   ä¾‹å¦‚ï¼š"è«‹ç”¨ä¸€æ®µè©±ç¸½çµ", "æ¢åˆ—å¼èªªæ˜å„ªç¼ºé»", "è£½ä½œä¸€å€‹æ¯”è¼ƒè¡¨æ ¼", "summarize in bullet points", "çµ¦æˆ‘é»åˆ—å¼æ¸…å–®" ç­‰ã€‚
*   **ä½¿ç”¨è€…åœ¨å•é¡Œä¸­æå‡ºçš„æ ¼å¼è¦æ±‚ï¼Œæ“æœ‰çµ•å°çš„æœ€é«˜å„ªå…ˆæ¬Šï¼Œå¿…é ˆè¦†è“‹æ‰€æœ‰å…¶ä»–é è¨­çš„æ ¼å¼æˆ–é¢¨æ ¼ã€‚**
*   å¦‚æœä½¿ç”¨è€…çš„è¦æ±‚éš±å«äº†æŸç¨®çµæ§‹ï¼ˆå¦‚è¦æ±‚åˆ—è¡¨ï¼‰ï¼Œè«‹ä½¿ç”¨ **æ¨™æº–ä¸”èªç¾©æ­£ç¢ºçš„ Markdown** ä¾†å¯¦ç¾ (ä¾‹å¦‚ï¼Œåˆ—è¡¨ä½¿ç”¨ `* ` æˆ– `1. `ï¼Œå¼·èª¿ä½¿ç”¨ `**ç²—é«”æ–‡å­—**`)ã€‚
*   **åš´æ ¼ç¢ºä¿ Markdown èªæ³•çš„æ­£ç¢ºæ€§ï¼š** `**ç²—é«”**` å¿…é ˆä½¿ç”¨å…©å€‹æ˜Ÿè™Ÿï¼Œ**çµ•å°é¿å…** ä½¿ç”¨å¤šé¤˜çš„æ˜Ÿè™Ÿ (å¦‚ `***æ¨™é¡Œ***`) æˆ–å–®å€‹æ˜Ÿè™Ÿ (`*å¼·èª¿*`) ä¾†è¡¨ç¤ºç²—é«”æˆ–æ¨™é¡Œã€‚

ğŸ’¡ **ä¾‹å¤–æƒ…æ³è™•ç†:**
å¦‚æœç¶“éä»”ç´°åˆ†æï¼Œä½  **ç¢ºèª** ä½¿ç”¨è€…çš„å•é¡Œæ–‡å­—ä¸­ **ç¢ºå¯¦æ²’æœ‰åŒ…å«ä»»ä½•æ˜ç¢ºçš„æ ¼å¼æŒ‡ä»¤** (å³ä½¿ `format_mode` è¢«è¨­ç‚º 'custom')ï¼Œé‚£éº¼è«‹ **å¿½ç•¥ `custom` æ¨¡å¼**ï¼Œä¸¦æ ¹æ“šæä¾›çš„ä¸Šä¸‹æ–‡ï¼Œä»¥ **æ¸…æ™°ã€æœ‰æ¢ç†çš„æ¨™æº–æ®µè½** å½¢å¼ï¼Œå®Œæ•´åœ°å›ç­”å•é¡Œå³å¯ã€‚

ğŸ“˜ **Conversation History:** {history}
ğŸ“„ **Retrieved Context:** {context}

â“ **User Question:**
{question}

ğŸ‘‡ **è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚çµ•å°å„ªå…ˆéµå¾ªä½¿ç”¨è€…å•é¡Œä¸­çš„æ ¼å¼è¦æ±‚ã€‚è‹¥ç„¡æ˜ç¢ºè¦æ±‚ï¼Œå‰‡ä»¥æ¨™æº–æ®µè½å›ç­”ã€‚è«‹ç¢ºä¿ Markdown èªæ³•ä½¿ç”¨æ­£ç¢ºã€‚**
"""
)
# --- æ¨¡æ¿ï¼šç ”ç©¶å ±å‘Šæ¨¡å¼ ---
RESEARCH_PROMPT_TEMPLATE = PromptTemplate(
    input_variables=["question", "context", "history", "format_mode"],
    template="""
You are a policy analyst and academic writer providing an evaluation in **Traditional Chinese**. Your output should be suitable for a formal academic or governmental report, using **Markdown headings** for structure.

ğŸ“Œ **Format Mode:** {format_mode}

ğŸš¨ **Formatting Guidelines (åš´æ ¼éµå®ˆ):**

*   **If `format_mode` is `custom`:** This means the user specified a custom output structure in their question. Follow the user's requested formatting **exactly**, maintaining a formal and academic tone. Use standard Markdown (`#`, `##`, `###`, `* list item`, `**bold**`) as appropriate unless the user specifies otherwise.
*   **If `format_mode` is `default`:** Follow the standard structured report format using **Markdown headings** as described below. Strictly adhere to the formatting rules.

    1.  **ä¸»æ¨™é¡Œ (Main Title):** ä½¿ç”¨ `#` (ä¸€å€‹äº•è™Ÿ) ä½œç‚ºæ•´å€‹å ±å‘Šçš„ä¸»æ¨™é¡Œ (è‹¥é©ç”¨)ã€‚
    2.  **ä¸»è¦ç« ç¯€æ¨™é¡Œ (Section Headers):** ä½¿ç”¨ `##` (å…©å€‹äº•è™Ÿ) æ¨™ç¤ºä¸»è¦ç« ç¯€ (ä¾‹å¦‚: `## ä¸€ã€æˆæ•ˆäº®é»`)ã€‚
    3.  **æ¬¡è¦ç« ç¯€/å°ç¯€æ¨™é¡Œ (Sub-section Headers):** ä½¿ç”¨ `###` (ä¸‰å€‹äº•è™Ÿ) æ¨™ç¤ºæ¬¡è¦ç« ç¯€æˆ–å°ç¯€ (ä¾‹å¦‚: `### 1. PM2.5 æ¿ƒåº¦è®ŠåŒ–`)ï¼Œè¦–éœ€è¦ä½¿ç”¨ã€‚
    4.  **å…§å®¹ (Content):** åœ¨æ¨™é¡Œä¸‹æ–¹ï¼Œä½¿ç”¨æ¸…æ™°çš„æ®µè½æ–‡å­—ã€‚
    5.  **åˆ—è¡¨ (Lists):** è‹¥éœ€æ¢åˆ—ï¼Œè«‹ä½¿ç”¨æ¨™æº– Markdown åˆ—è¡¨ï¼Œä»¥ `* ` æˆ– `- ` é–‹é ­ (ä¾‹å¦‚: `* é …ç›®ä¸€`)ã€‚åˆ—è¡¨é …ç›®æœ¬èº«æ‡‰ç‚º**ç´”æ–‡å­—**ï¼Œé™¤éç‰¹å®šé—œéµå­—éœ€è¦**ç²—é«”** (`**å¼·èª¿è©**`)ã€‚**ä¸è¦å°‡æ•´å€‹åˆ—è¡¨é …ç›®åŠ ç²—**ã€‚
    6.  **é–“è· (Spacing):** åœ¨ä¸»æ¨™é¡Œå’Œç¬¬ä¸€å€‹ç« ç¯€æ¨™é¡Œä¹‹é–“ã€ä»¥åŠå„å€‹ç« ç¯€æ¨™é¡Œ (`##`) ä¹‹é–“ä¿ç•™ä¸€å€‹ç©ºè¡Œã€‚
    7.  **èªè¨€èˆ‡é¢¨æ ¼ (Language & Tone):** å…¨æ–‡ä½¿ç”¨**ç¹é«”ä¸­æ–‡**ã€‚èªæ°£å¿…é ˆæ­£å¼ã€å®¢è§€ã€å­¸è¡“ã€‚**ç¦æ­¢**ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿ (emojis)ã€éæ¨™æº–çš„ Markdown æ¨£å¼ (é™¤äº†æŒ‡ç¤ºçš„ `#`, `##`, `###`, `*`, `**`) æˆ–å£èªåŒ–/éæ­£å¼èªè¨€ã€‚

ğŸ“˜ **Conversation History:**
{history}

ğŸ“˜ **Document Context:**
{context}

ğŸ§¾ **User's Question:**
{question}

ğŸ‘‡ **è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”:**
*   è‹¥ `format_mode` æ˜¯ `custom`ï¼Œè«‹å®Œå…¨éµå¾ªä½¿ç”¨è€…åœ¨å•é¡Œä¸­å®šç¾©çš„æ ¼å¼ã€‚
*   è‹¥ `format_mode` æ˜¯ `default`ï¼Œè«‹åš´æ ¼éµå®ˆä¸Šè¿°ä½¿ç”¨ **Markdown æ¨™é¡Œ (`#`, `##`, `###`)** çš„çµæ§‹åŒ–å ±å‘Šæ ¼å¼ã€‚**ç²—é«” (`**`) åƒ…ç”¨æ–¼å…§æ–‡ç‰¹å®šè©èªå¼·èª¿ï¼Œä¸å¯ç”¨æ–¼æ¨™é¡Œæˆ–æ•´å€‹åˆ—è¡¨é …ç›®ã€‚**

ğŸ“ **EXAMPLE OUTPUT FORMAT (when `format_mode` is "default"):**

# é«˜é›„å°æ¸¯ç©ºæ°£æ±¡æŸ“è­°é¡Œåˆ†æå ±å‘Š

## ä¸€ã€æˆæ•ˆäº®é»
é«˜é›„å°æ¸¯å€ç‚ºå°ç£é‡è¦çš„å·¥æ¥­é‡é®ï¼Œéå»é•·æœŸé­å—çŸ³åŒ–æ¥­èˆ‡é‡å·¥æ¥­æ’æ”¾æ‰€å¸¶ä¾†çš„ç©ºæ°£æ±¡æŸ“ã€‚è‡ªæ”¿åºœå¯¦æ–½ç©ºæ±¡é˜²åˆ¶å¼·åŒ–æ–¹æ¡ˆä»¥ä¾†ï¼Œå·²é€æ­¥è¦‹åˆ°æˆæœï¼š

*   PM2.5 å¹´å‡æ¿ƒåº¦ä¸‹é™ï¼š2023å¹´ï¼Œå°æ¸¯å€PM2.5æ¿ƒåº¦é¦–æ¬¡é™è‡³15Î¼g/mÂ³ä»¥ä¸‹ï¼Œç¬¦æ‡‰åœ‹å®¶æ¨™æº–ã€‚
*   é«˜æ±¡æŸ“å·¥å» æ”¹å–„ï¼šå¤šå®¶é«˜æ±¡æŸ“äº‹æ¥­å®Œæˆé‹çˆè¨­å‚™æ›´æ–°æˆ–æ±¡æŸ“é˜²åˆ¶è¨­æ–½å¼·åŒ–ã€‚
*   åœ¨åœ°åƒèˆ‡æå‡ï¼šé€éç¤¾å€è«–å£‡ã€æ ¡åœ’æ•™è‚²åŠUSRå”ä½œï¼Œå°æ¸¯å±…æ°‘åƒèˆ‡ç©ºå“æ”¹å–„æ´»å‹•äººæ•¸é¡¯è‘—æå‡ï¼Œå±•ç¾åœ°æ–¹å…±æ²»çš„æ½›èƒ½ã€‚

é€™äº›æˆæœèªªæ˜æ”¿ç­–å…·å‚™åˆæ­¥æ•ˆç›Šï¼Œä¹Ÿé¡¯ç¤ºç¤¾å€åŠ›é‡åœ¨ç’°å¢ƒæ²»ç†ä¸­æ—¥ç›Šé—œéµã€‚

## äºŒã€ä¸»è¦æŒ‘æˆ°èˆ‡é™åˆ¶
å„˜ç®¡å·²æœ‰é¡¯è‘—é€²å±•ï¼Œå°æ¸¯å€çš„ç©ºæ°£æ±¡æŸ“å•é¡Œä»å­˜åœ¨ä¸‹åˆ—æŒ‘æˆ°ï¼š

*   çµæ§‹æ€§æ±¡æŸ“æºæŒçºŒå­˜åœ¨ï¼šå°æ¸¯æ“æœ‰å¤šå€‹å¤§å‹çŸ³åŒ–å·¥æ¥­å€ï¼Œæ±¡æŸ“ç‰©ç¸½é‡åŸºæ•¸é«˜ï¼Œä½¿å¾—å–®ä¸€æ”¹å–„æªæ–½é›£ä»¥å°ç©ºå“ç”¢ç”ŸåŠ‡çƒˆæ”¹è®Šã€‚
*   åœ°å½¢èˆ‡æ°£å€™åŠ£å‹¢ï¼šèƒŒé¢¨å´æ•ˆæ‡‰èˆ‡æ°£å€™æ¢ä»¶ä½¿æ±¡æŸ“ç‰©æ˜“æ»¯ç•™ï¼ŒåŠ é‡å±€éƒ¨æ±¡æŸ“æ¿ƒåº¦ã€‚
*   æ”¿ç­–å”ä½œè½å·®ï¼šä¸­å¤®èˆ‡åœ°æ–¹åœ¨æ±¡æŸ“ç†±å€è³‡æ–™æ•´åˆèˆ‡æ‡‰è®Šä½œç‚ºä¸Šï¼Œä»é¡¯ä¸è¶³ï¼Œå°è‡´åæ‡‰æ™‚é–“å»¶é²ã€ç¼ºä¹å³æ™‚èª¿æ§åŠ›ã€‚
*   å…¬æ°‘å‹•èƒ½ä¸è¶³ï¼šéƒ¨åˆ†å±…æ°‘å°ç©ºæ±¡è­°é¡Œå·²ç”¢ç”Ÿã€Œç¿’æ…£æ€§éº»ç—ºã€ï¼Œç¼ºä¹ä¸»å‹•ç›£ç£èˆ‡è¡Œå‹•åƒèˆ‡ã€‚

## ä¸‰ã€æ”¿ç­–å»ºè­°èˆ‡æ”¹é€²æ–¹å‘
ç‚ºæœ‰æ•ˆæ·±åŒ–æ²»ç†æˆæ•ˆï¼Œå»ºè­°å¯å¾ä»¥ä¸‹ä¸‰å€‹ç­–ç•¥æ¨å‹•ï¼š

### 1. æ“´å¤§æ„Ÿæ¸¬èˆ‡è³‡è¨Šå…¬é–‹
*   å»ºç«‹é«˜å¯†åº¦æ„Ÿæ¸¬ç¶²ï¼Œå¼·åŒ–ç§»å‹•å¼ç›£æ¸¬ã€‚
*   ç™¼å±•å³æ™‚æ±¡æŸ“è¦–è¦ºåŒ–å¹³å°ï¼Œæå‡å…¬çœ¾é¢¨éšªæ„è­˜ã€‚

### 2. ç”¢æ¥­è½‰å‹èˆ‡æ’æ”¾ç¸½é‡æ§ç®¡
*   æ¨è¡Œã€Œç¸½é‡ç®¡åˆ¶ + å·®åˆ¥è²»ç‡ã€ï¼Œå¼•å°æ±¡æŸ“æ¥­è€…å‡ç´šã€‚
*   é¼“å‹µæ½”æ·¨èƒ½æºä½¿ç”¨èˆ‡ç¢³æ’é€æ˜æ­éœ²ã€‚

### 3. å¼·åŒ–ç¤¾å€å…±æ²»èˆ‡ç’°å¢ƒæ•™è‚²
*   æ•´åˆUSRè¨ˆç•«èˆ‡åœ°æ–¹èª²ç¨‹ï¼Œç™¼å±•ç©ºæ±¡è³‡æ–™è§£è®€èˆ‡å€¡è­°èƒ½åŠ›ã€‚
*   å»ºç«‹ç¤¾å€åƒèˆ‡åˆ¶åº¦ï¼Œå¦‚å±…æ°‘ç©ºå“æœƒè­°ã€æ”¿ç­–åƒèˆ‡ç®¡é“ç­‰ã€‚

## å››ã€ç¸½é«”è§€å¯Ÿèˆ‡è©•è«–
å°æ¸¯ç©ºæ±¡å•é¡Œçš„æ²»ç†é›£åº¦æºè‡ªçµæ§‹æ€§æ·±å±¤å› ç´ ï¼ŒçŸ­æœŸå…§é›£ä»¥å¾¹åº•é€†è½‰ã€‚ç„¶è€Œï¼Œå·²æœ‰è·¡è±¡é¡¯ç¤ºï¼Œåªè¦æ”¿ç­–æŒçºŒæ¨é€²ä¸¦å¼·åŒ–åœ°æ–¹å…±æ²»ï¼Œå°‡æœ‰æ©Ÿæœƒè½‰å±ç‚ºæ©Ÿã€‚ç‰¹åˆ¥æ˜¯çµåˆ**ç§‘æŠ€ç›£æ¸¬**èˆ‡**æ°‘çœ¾è¡Œå‹•åŠ›**ï¼Œèƒ½æ§‹ç¯‰å‡ºä¸€å¥—é©åˆå°ç£é‡å·¥æ¥­éƒ½å¸‚çš„æ°¸çºŒæ²»ç†æ¨¡å¼ã€‚

## äº”ã€çµè«–
é«˜é›„å°æ¸¯çš„ç©ºæ±¡æ”¹å–„æ­·ç¨‹æ˜¯å°ç£å·¥æ¥­å€ç’°å¢ƒæ²»ç†çš„ç¸®å½±ã€‚æŒçºŒè½å¯¦ã€Œç§‘æŠ€å°å‘ + å…¬æ°‘åƒèˆ‡ + æ³•åˆ¶æ”¹é©ã€ä¸‰ä½ä¸€é«”çš„ç­–ç•¥ï¼Œå°‡æ˜¯æœªä¾†æå‡ç©ºå“èˆ‡å¥åº·ç¦ç¥‰çš„é—œéµæ–¹å‘ã€‚
"""
)


DEFAULT_PROMPT_OPTIONS = [
    STRUCTURED_LIST_PROMPT,
    HIERARCHICAL_BULLETS_PROMPT,
    PARAGRAPH_EMOJI_LEAD_PROMPT
]

# --- LLM Configuration and Loading ---
common_llm_config = { "temperature": 0.1, "top_p": 0.8 }
# è€ƒæ…®å°‡æ¨¡å‹åˆ—è¡¨è¨­ç‚ºç’°å¢ƒè®Šæ•¸æˆ–é…ç½®æ–‡ä»¶
SUPPORTED_MODELS = [
    "qwen2.5:14b",
    "gemma3:12b",
    "gemma3:12b-it-q4_K_M",
    "qwen2.5:14b-instruct-q5_K_M",
    "mistral-small3.1:24b-instruct-2503-q4_K_M",
    "phi4-mini-reasoning:3.8b",
]
# é è¨­æ¨¡å‹ï¼Œå¦‚æœå‰ç«¯æ²’å‚³æˆ–å‚³äº†ä¸æ”¯æ´çš„
DEFAULT_MODEL = "gemma3:12b-it-q4_K_M" # æˆ–è€…é¸æ“‡ä¸€å€‹æœ€å¸¸ç”¨çš„

@lru_cache(maxsize=5) # æ ¹æ“šä½ åŒæ™‚ä½¿ç”¨çš„æ¨¡å‹æ•¸é‡èª¿æ•´ç·©å­˜å¤§å°
def get_model(model_name: str) -> Optional[OllamaLLM]:
    """è¼‰å…¥ä¸¦ç·©å­˜ Ollama æ¨¡å‹"""
    if model_name not in SUPPORTED_MODELS:
        logging.warning(f"âš ï¸ è«‹æ±‚çš„æ¨¡å‹ '{model_name}' ä¸åœ¨æ”¯æ´åˆ—è¡¨ä¸­ï¼Œå°‡ä½¿ç”¨é è¨­æ¨¡å‹ '{DEFAULT_MODEL}'ã€‚")
        model_name = DEFAULT_MODEL

    try:
        logging.info(f"â³ æ­£åœ¨è¼‰å…¥æˆ–ç²å–ç·©å­˜çš„æ¨¡å‹: {model_name}...")
        # å¢åŠ è«‹æ±‚è¶…æ™‚æ™‚é–“ï¼Œé˜²æ­¢å¤§æ¨¡å‹è¼‰å…¥éä¹…
        model = OllamaLLM(model=model_name, **common_llm_config, request_timeout=300.0)
        # é€²è¡Œä¸€æ¬¡ç°¡å–®çš„èª¿ç”¨ä»¥ç¢ºä¿æ¨¡å‹å¯ç”¨ (æˆ–é ç†±)
        _ = model.invoke("è«‹åšå€‹è‡ªæˆ‘ä»‹ç´¹")
        logging.info(f"âœ… æ¨¡å‹ {model_name} è¼‰å…¥ä¸¦æ¸¬è©¦æˆåŠŸ")
        return model
    except Exception as e:
        logging.error(f"âŒ è¼‰å…¥æˆ–æ¸¬è©¦æ¨¡å‹ {model_name} å¤±æ•—: {str(e)}", exc_info=True)
        return None

# --- Rate Limiting Middleware ---
@app.middleware("http")
async def rate_limit_middleware(request: Request, call_next):
    # åªå° /chat è·¯ç”±é€²è¡Œé™åˆ¶
    if "/chat" not in str(request.url):
        return await call_next(request)

    client_ip = request.client.host if request.client else "unknown"
    current_time = time.time()

    # æ¸…ç†éæœŸçš„æ™‚é–“æˆ³
    request_timestamps = request_counters.get(client_ip, [])
    valid_timestamps = [t for t in request_timestamps if current_time - t < 60] # ä¿ç•™ 60 ç§’å…§çš„è¨˜éŒ„

    # æª¢æŸ¥è«‹æ±‚æ¬¡æ•¸ (ä¾‹å¦‚ï¼Œæ¯åˆ†é˜ 30 æ¬¡)
    rate_limit_count = 30
    if len(valid_timestamps) >= rate_limit_count:
        logging.warning(f"ğŸš¦ é€Ÿç‡é™åˆ¶è§¸ç™¼: IP {client_ip} å·²é”åˆ° {rate_limit_count}/åˆ†é˜ çš„é™åˆ¶ã€‚")
        from fastapi.responses import JSONResponse
        return JSONResponse(
            status_code=429, # Too Many Requests
            content={"error": "è«‹æ±‚éæ–¼é »ç¹ï¼Œè«‹ç¨å¾Œå†è©¦"}
        )

    # æ·»åŠ ç•¶å‰æ™‚é–“æˆ³
    valid_timestamps.append(current_time)
    request_counters[client_ip] = valid_timestamps

    response = await call_next(request)
    return response

# --- Pydantic Models ---
class ChatRequest(BaseModel):
    session_id: str
    question: str
    model: Optional[str] = DEFAULT_MODEL # å…è¨±å‰ç«¯ä¸å‚³ï¼Œä½¿ç”¨é è¨­å€¼
    prompt_mode: str = "default"

class FeedbackRequest(BaseModel):
    session_id: str
    question: str
    model: str
    original_answer: str
    user_expected_question: Optional[str] = None
    user_expected_answer: str

# --- Directory Setup ---
try:
    FEEDBACK_SAVE_PATH.mkdir(parents=True, exist_ok=True)
    QA_LOG_PATH.mkdir(parents=True, exist_ok=True)
    # æª¢æŸ¥å¯«å…¥æ¬Šé™ (é›–ç„¶ mkdir é€šå¸¸æœƒæ‹‹å‡ºéŒ¯èª¤ï¼Œä½†å¤šä¸€å±¤ä¿éšª)
    if not os.access(FEEDBACK_SAVE_PATH, os.W_OK):
        logging.error(f"âŒ æ¬Šé™ä¸è¶³ï¼šç„¡æ³•å¯«å…¥ç›®éŒ„ {FEEDBACK_SAVE_PATH}")
    if not os.access(QA_LOG_PATH, os.W_OK):
        logging.error(f"âŒ æ¬Šé™ä¸è¶³ï¼šç„¡æ³•å¯«å…¥ç›®éŒ„ {QA_LOG_PATH}")
except Exception as e:
    logging.error(f"âŒ å»ºç«‹å„²å­˜ç›®éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)


# ==============================================================================
#  Post-processing Function (V7)
# ==============================================================================
def post_process_answer(answer: str) -> str:
    """
    å° LLM çš„åŸå§‹å›ç­”é€²è¡Œå¾Œè™•ç†ï¼Œæ¸…ç†æ ¼å¼ä¸¦è½‰æ› Markdown ç‚º HTMLã€‚
    ç‰ˆæœ¬ï¼šV9.1 (æ•´åˆæ¨™é¡Œè½‰æ›, ä¿®æ­£æ˜Ÿè™Ÿè™•ç†, ç§»é™¤å…ƒèªè¨€/ä¾†æºè©•è«–, è™•ç†æ¨™è¨˜å¾Œç²—é«”, ç§»é™¤ç‰¹å®šæ‹¬è™Ÿè¨»é‡‹)
    """
    original_answer_before_cleanup = answer
    try:
        logging.debug(f"Post-processing V9.1 - Input: {answer[:200]}...")

        # === V8/V9.1 Start: ç§»é™¤å…ƒèªè¨€ã€ä¾†æºè©•è«–å’Œç‰¹å®šèªå¥/è¨»é‡‹ ===
        # ç§»é™¤é–‹é ­å¹²æ“¾èªå¥ (V8)
        patterns_to_remove_at_start = [
            r"^\s*æ ¹æ“šæä¾›çš„æ–‡æœ¬[,ï¼Œ]?\s*", r"^\s*æ–‡æœ¬æŒ‡å‡º[,ï¼Œ]?\s*",
            r"^\s*æ–‡æœ¬æœª(?:ç›´æ¥)?æåŠ(?:ï¼Œ|,)ä½†å¯ä»¥æ¨æ–·[,ï¼Œ]?\s*",
            r"^\s*ä»¥ä¸‹å°‡.*?æ ¼å¼å‘ˆç¾ï¼š?\s*", r"^\s*é€™ä»½æ–‡ä»¶æåˆ°(?:äº†)?",
            r"^\s*å¥½çš„[,ï¼Œ]?\s*", r"^\s*æ˜¯çš„[,ï¼Œ]?\s*",
            r"^\s*Here is the.*?you requested[:.]?\s*",
        ]
        for pattern in patterns_to_remove_at_start:
            original_len = len(answer)
            answer = re.sub(pattern, "", answer, count=1, flags=re.IGNORECASE | re.MULTILINE).lstrip()
            if len(answer) != original_len:
                 logging.info(f"Post-processing V9.1 - Removed start pattern matching: '{pattern}'")

        # å§”å©‰æ›¿æ›ç‰¹å®šå¥å­ (V8)
        def replace_lack_of_enforcement(match):
            logging.info("Post-processing V9.1 - Replacing 'lack of enforcement' phrase.")
            return "åŸ·æ³•æ©Ÿåˆ¶çš„æœ‰æ•ˆæ€§æ˜¯å½±éŸ¿æ”¹å–„é€Ÿåº¦çš„é—œéµå› ç´ ã€‚"
        answer = re.sub(
            r"(\n\s*|\A\s*)(ç¼ºä¹åš´æ ¼çš„åŸ·æ³•æ©Ÿåˆ¶.*?)(?:\s*ã€‚|\s*\n|\s*$)",
            lambda m: m.group(1) + replace_lack_of_enforcement(m),
            answer, flags=re.MULTILINE
        )
        answer = re.sub(
            r"(\n\s*|\A\s*)(é€™å¯èƒ½æ¶‰åŠ.*?(?:ã€‚|\n|$))", r"\1",
            answer, flags=re.MULTILINE
        )
        logging.debug("Post-processing V9.1 - Specific sentence replacements: Done")

        # --- æ–°å¢ V9.1: ç§»é™¤æ¨™é¡Œä¸­ç‰¹å®šçš„æ‹¬è™Ÿè¨»é‡‹ ---
        # ç²¾ç¢ºåŒ¹é… "(æ–‡æœ¬æœªç›´æ¥æåŠï¼Œä½†å¯æ¨æ¸¬)"ï¼ŒåŒ…å«å‰å¾Œå¯èƒ½å­˜åœ¨çš„ç©ºæ ¼
        # ä½¿ç”¨ re.escape ä¾†è™•ç†æ‹¬è™Ÿçš„ç‰¹æ®Šå­—ç¬¦
        phrase_to_remove = "(æ–‡æœ¬æœªç›´æ¥æåŠï¼Œä½†å¯æ¨æ¸¬)"
        # å»ºç«‹æ­£å‰‡è¡¨é”å¼ï¼ŒåŒ¹é…æ‹¬è™Ÿæœ¬èº«ä»¥åŠå‰å¾Œçš„ç©ºæ ¼
        # \s* åŒ¹é…é›¶å€‹æˆ–å¤šå€‹ç©ºæ ¼ï¼Œ re.escape è™•ç†æ‹¬è™Ÿ
        unwanted_phrase_pattern = r'\s*' + re.escape(phrase_to_remove) + r'\s*'
        original_len_v9_1 = len(answer)
        # ç›´æ¥æ›¿æ›ç‚ºç©ºå­—ä¸²ï¼Œç›¸ç•¶æ–¼ç§»é™¤
        answer = re.sub(unwanted_phrase_pattern, '', answer)
        if len(answer) != original_len_v9_1:
            logging.info(f"Post-processing V9.1 - Removed specific parenthetical phrase: '{phrase_to_remove}'")
        # === V8/V9.1 End ===


        # --- æ¸…ç†æ½›åœ¨çš„ã€ç”± LLM éŒ¯èª¤æ·»åŠ çš„æç¤ºè©æ®˜ç•™ (V8) ---
        markers_to_remove = [
            "ğŸ“˜ Conversation History:", "ğŸ“„ Retrieved Context:", "â“ User Question:",
            "ğŸ‘‡ Please write your answer", "ğŸ“ EXAMPLE OUTPUT FORMAT",
            "You are a helpful assistant", "You are a policy analyst"
        ]
        for marker in markers_to_remove:
            if marker in answer:
                logging.warning(f"âš ï¸ åœ¨å¾Œè™•ç†ä¸­ç™¼ç¾æ®˜ç•™æ¨™è¨˜ '{marker}'ï¼Œå°‡å…¶ç§»é™¤ã€‚")
                answer_parts = answer.split(marker, 1)
                if len(answer_parts) > 0:
                    answer = answer_parts[0].strip()
        logging.debug("Post-processing V9.1 - Marker removal: Done")

        # â”€â”€ V5ï¼šå¸¸è¦æ˜Ÿè™Ÿæ¸…ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (ç°¡åŒ–ç‰ˆï¼Œå› ç‚ºV9æœƒè™•ç†æ¨™è¨˜å¾Œç²—é«”)
        answer = re.sub(r'\*{3,}', '**', answer)
        answer = answer.replace('** ', '**').replace(' **', '**')
        logging.debug("Post-processing V9.1 - V5 Asterisk cleanup (basic): Done")

        # â”€â”€ V6ï¼šbullet è½‰æ› â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        answer = re.sub(r'(?m)^\s*([*-])\s+', 'â€¢ ', answer)
        logging.debug("Post-processing V9.1 - V6 Bullet conversion: Done")

        # === V9: ç§»é™¤æ¨™è¨˜/å†’è™Ÿå¾Œä¸æ­£ç¢ºçš„ç²—é«” ===
        original_len_v9 = len(answer)
        answer = re.sub(
            r'(?m)^(\s*(?:(?:[^:\n]+:)|[â€¢]|\d+\.)\s*)\*\*(.+?)\*\*', # å‡è¨­ V6 å·²è½‰ç‚º â€¢
            # r'(?m)^(\s*(?:(?:[^:\n]+:)|[â€¢*-]|\d+\.)\s*)\*\*(.+?)\*\*', # å¦‚æœéœ€è¦è™•ç† * æˆ– -
            r'\1 \2',
            answer
        )
        if len(answer) != original_len_v9:
            logging.info("Post-processing V9.1 - Removed incorrect bolding immediately after list markers or colons.")
        # === V9 End ===


        # â”€â”€ V7ï¼šMarkdown æ¨™é¡Œè½‰æ›ç‚º HTML â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if re.search(r'(?m)^#{1,3}[^#\s]', answer):
             logging.warning(f"Post-processing V9.1 - Detected potential heading missing space before conversion: {answer[:200]}")
        logging.info(f"Post-processing V9.1 - Before heading conversion: {answer[:200]}")
        answer = re.sub(r'(?m)^###\s+(.*?)\s*$', r'<h3>\1</h3>', answer)
        answer = re.sub(r'(?m)^##\s+(.*?)\s*$', r'<h2>\1</h2>', answer)
        answer = re.sub(r'(?m)^#\s+(.*?)\s*$', r'<h1>\1</h1>', answer)
        logging.info(f"Post-processing V9.1 - After heading conversion: {answer[:200]}")

        # â”€â”€ V7 (çºŒ)ï¼šæœ€çµ‚ Markdown è½‰æ›èˆ‡æ¸…ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        answer = re.sub(r'\*\*(.+?)\*\*', r'<strong>\1</strong>', answer)
        logging.debug("Post-processing V9.1 - Final ** to <strong> conversion: Done")
        # (å¯é¸ç§»é™¤å–®æ˜Ÿè™Ÿ)
        # answer = answer.replace('*', '')

        # --- æ¸…ç†å¤šé¤˜ç©ºè¡Œå’Œé¦–å°¾ç©ºæ ¼ ---
        answer = answer.strip()
        answer = re.sub(r'\n{3,}', '\n\n', answer)
        logging.debug("Post-processing V9.1 - Trailing newline cleanup: Done")

        # æœ€çµ‚æª¢æŸ¥ï¼Œå¦‚æœè™•ç†å¾Œè®Šç‚ºç©ºå­—ä¸²ï¼Œæ¢å¾©åŸå§‹ç­”æ¡ˆ
        if not answer.strip() and original_answer_before_cleanup.strip():
            logging.warning("Post-processing V9.1 resulted in empty string, reverting to original.")
            return original_answer_before_cleanup.strip()

        if answer != original_answer_before_cleanup:
            logging.info("Applied post-processing V9.1 (Parenthetical removal, etc.).")
        else:
            logging.info("Post-processing V9.1 did not significantly change the answer.")

        return answer

    except Exception as post_process_error:
        logging.error(f"âŒ Post-processing V9.1 failed: {post_process_error}", exc_info=True)
        return original_answer_before_cleanup.strip()

# ==============================================================================
#  API Endpoints
# ==============================================================================

@app.post("/chat")
async def chat(req: ChatRequest):
    start_all = time.time()
    session_id = req.session_id
    question = req.question.strip() # æ¸…ç†ç”¨æˆ¶å•é¡Œçš„é¦–å°¾ç©ºæ ¼
    selected_model = req.model if req.model in SUPPORTED_MODELS else DEFAULT_MODEL
    prompt_mode = req.prompt_mode if req.prompt_mode in ["default", "research"] else "default"

    # å¦‚æœå•é¡Œç‚ºç©ºï¼Œç›´æ¥è¿”å›
    if not question:
        logging.warning("âš ï¸ Received empty question.")
        return {"error": "å•é¡Œä¸èƒ½ç‚ºç©º"}

    # --- ç²å– LLM å’Œ VectorDB ---
    llm = get_model(selected_model)
    if llm is None:
        # get_model å…§éƒ¨å·²ç¶“ log ééŒ¯èª¤
        return {"error": f"ç„¡æ³•è¼‰å…¥èªè¨€æ¨¡å‹ '{selected_model}'ï¼Œè«‹ç¨å¾Œå†è©¦æˆ–è¯ç¹«ç®¡ç†å“¡ã€‚"}

    if vectordb is None:
        logging.error("âŒ /chat error: Vector database not available.")
        return {"error": "å‘é‡è³‡æ–™åº«ä¸å¯ç”¨ï¼Œè«‹è¯ç¹«ç®¡ç†å“¡ã€‚"}

    # --- æ ¼å¼æ¨¡å¼æª¢æ¸¬ ---
    detected_format_mode = detect_format_mode(question)
    # å¦‚æœ prompt_mode æ˜¯ researchï¼Œå‰‡ format_mode ä¹Ÿè·Ÿéš¨æª¢æ¸¬çµæœ
    # å¦‚æœ prompt_mode æ˜¯ defaultï¼Œå‰‡ format_mode ä¹Ÿè·Ÿéš¨æª¢æ¸¬çµæœ
    # ï¼ˆç›®å‰çœ‹èµ·ä¾† format_mode ç¸½æ˜¯è·Ÿéš¨ detected_format_modeï¼‰
    format_mode = detected_format_mode

    logging.info(f"ğŸš€ Request - Session: {session_id}, Model: {selected_model}, PromptMode(Frontend): {prompt_mode}, FormatMode(Used): {format_mode}")
    logging.info(f"â“ Question: {question}")

    # --- History Management ---
    history = chat_memory.get(session_id, [])
    # æ¸…ç†éé•·çš„æ­·å²è¨˜éŒ„
    if len(history) > MAX_HISTORY_PER_SESSION:
        history = history[-MAX_HISTORY_PER_SESSION:]
        chat_memory[session_id] = history
        logging.info(f"Session {session_id} history truncated to {MAX_HISTORY_PER_SESSION} entries.")
    history_text = "\n".join([f"ä½¿ç”¨è€…: {q}\nåŠ©ç†: {a}" for q, a in history]) if history else "ç„¡æ­·å²å°è©±ç´€éŒ„ã€‚"

    # --- Retrieval ---
    start_retrieve = time.time()
    # è€ƒæ…®å°‡æª¢ç´¢åƒæ•¸è¨­ç‚ºå¯é…ç½®
    retriever_k = 15
    retriever_fetch_k = 25
    retriever_lambda_mult = 0.9
    retriever = vectordb.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": retriever_k,
            "fetch_k": retriever_fetch_k,
            "lambda_mult": retriever_lambda_mult
        }
    )
    context = ""
    docs = []
    try:
        # Langchain çš„ BGE HF Embeddings é€šå¸¸ä¸éœ€è¦ "query: " å‰ç¶´
        # ä½†å¦‚æœä½ çš„æ•¸æ“šåº«æ˜¯é€™æ¨£æ§‹å»ºçš„ï¼Œå‰‡ä¿ç•™
        # question_for_retrieval = f"query: {question}"
        question_for_retrieval = question
        docs = retriever.invoke(question_for_retrieval)
        if not docs:
            logging.warning(f"âš ï¸ Retrieval warning: No relevant documents found for question: '{question}'")
            context = "æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„èƒŒæ™¯è³‡æ–™ã€‚" # æˆ–è€…è¿”å›ä¸€å€‹æç¤ºä¿¡æ¯
        else:
            context = "\n\n".join([doc.page_content for doc in docs])
            logging.info(f"Retrieved {len(docs)} documents.")
        end_retrieve = time.time()
        logging.info(f"â±ï¸ Retrieval time: {end_retrieve - start_retrieve:.2f}s")
        logging.debug(f"Retrieved context snippet: {context[:200]}...")
    except Exception as e:
        logging.error(f"âŒ Retrieval error for question '{question}': {str(e)}", exc_info=True)
        # å³ä½¿æª¢ç´¢å¤±æ•—ï¼Œä¹Ÿå¯ä»¥è€ƒæ…®ç¹¼çºŒï¼ˆä¸å¸¶ä¸Šä¸‹æ–‡ï¼‰ï¼Œæˆ–è€…è¿”å›éŒ¯èª¤
        return {"error": f"å‘é‡è³‡æ–™åº«æª¢ç´¢éŒ¯èª¤ï¼š{str(e)}"}

    # --- Prompt Template Selection ---
    selected_template = None
    template_name = "N/A"
    if prompt_mode == "research":
        selected_template = RESEARCH_PROMPT_TEMPLATE
        # ç ”ç©¶æ¨¡å¼ä¸‹ä¹Ÿå€åˆ† default/custom æ ¼å¼
        template_name = f"Research Report ({'User Format' if format_mode == 'custom' else 'Default Format'})"
    elif format_mode == "custom":
        selected_template = CUSTOM_FORMAT_BASE_PROMPT
        template_name = "Custom Format Request"
    else: # prompt_mode is default and format_mode is default
        selected_template = random.choice(DEFAULT_PROMPT_OPTIONS)
        # ç¢ºå®šéš¨æ©Ÿé¸æ“‡çš„æ¨¡æ¿åç¨±
        if selected_template == STRUCTURED_LIST_PROMPT: template_name = "Default Style (Random: Structured List)"
        elif selected_template == HIERARCHICAL_BULLETS_PROMPT: template_name = "Default Style (Random: Hierarchical Bullets)"
        elif selected_template == PARAGRAPH_EMOJI_LEAD_PROMPT: template_name = "Default Style (Random: Paragraph Emoji Lead)"
        else: template_name = "Default Style (Random: Unknown)"
    logging.info(f"Using template: {template_name}")

    # --- Prompt Formatting ---
    try:
        prompt = selected_template.format(
            context=context,
            question=question,
            history=history_text,
            format_mode=format_mode
        )
        logging.debug(f"Formatted Prompt snippet: {prompt[:300]}...")
    except KeyError as e:
        logging.error(f"âŒ Prompt formatting error: Missing key {e} in template '{template_name}'", exc_info=True)
        return {"error": f"å…§éƒ¨éŒ¯èª¤ï¼šæ ¼å¼åŒ–æç¤ºè©æ™‚ç¼ºå°‘å¿…è¦è³‡è¨Š {e}"}
    except Exception as e:
        logging.error(f"âŒ Unexpected prompt formatting error: {str(e)}", exc_info=True)
        return {"error": f"å…§éƒ¨éŒ¯èª¤ï¼šæ ¼å¼åŒ–æç¤ºè©æ™‚ç™¼ç”Ÿæ„å¤–éŒ¯èª¤"}

    # --- LLM Invocation & Post-processing ---
    final_answer = None # åˆå§‹åŒ–æœ€çµ‚ç­”æ¡ˆ
    llm_attempts = 0
    start_llm_total = time.time()

    while llm_attempts <= MAX_LLM_RETRIES:
        try:
            start_llm_attempt = time.time()
            logging.info(f"ğŸ§  Calling LLM '{selected_model}' (Attempt {llm_attempts + 1}/{MAX_LLM_RETRIES + 1}, Style: {template_name})...")

            # èª¿ç”¨ LLM
            raw_answer = llm.invoke(prompt)
            llm_answer_stripped = raw_answer.strip() # å»é™¤é¦–å°¾ç©ºæ ¼

            logging.info(f"LLM Raw Output (Attempt {llm_attempts + 1}, before post-processing): {llm_answer_stripped[:200]}...")

            # <<< èª¿ç”¨å¾Œè™•ç†å‡½æ•¸ >>>
            processed_answer = post_process_answer(llm_answer_stripped)

            logging.info(f"Final Answer (Attempt {llm_attempts + 1}, after post-processing): {processed_answer[:200]}...")

            end_llm_attempt = time.time()
            logging.info(f"â±ï¸ LLM response time (Attempt {llm_attempts + 1}): {end_llm_attempt - start_llm_attempt:.2f}s")

            # æª¢æŸ¥è™•ç†å¾Œçš„ç­”æ¡ˆæ˜¯å¦ç‚ºç©ºæˆ–åƒ…åŒ…å«ç©ºæ ¼
            if not processed_answer or processed_answer.isspace():
                 logging.warning(f"âš ï¸ LLM Attempt {llm_attempts + 1} resulted in empty answer after processing.")
                 # å¯ä»¥é¸æ“‡åœ¨é€™è£¡é‡è©¦æˆ–æ‹‹å‡ºéŒ¯èª¤
                 raise ValueError("LLM returned empty or whitespace-only answer after processing.")

            final_answer = processed_answer # å°‡æˆåŠŸè™•ç†çš„çµæœè³¦å€¼çµ¦æœ€çµ‚ç­”æ¡ˆ

            # === LLM èª¿ç”¨ä¸¦è™•ç†æˆåŠŸï¼Œè·³å‡ºé‡è©¦è¿´åœˆ ===
            break

        except Exception as e:
            logging.error(f"âŒ LLM invocation or processing error (Attempt {llm_attempts + 1}): {str(e)}", exc_info=True)
            llm_attempts += 1
            if llm_attempts <= MAX_LLM_RETRIES:
                logging.info(f"ğŸ”„ Retrying LLM call ({llm_attempts}/{MAX_LLM_RETRIES + 1})...")
                time.sleep(1) # é‡è©¦å‰ç¨ç­‰
            else:
                logging.error(f"âŒ LLM invocation failed after {MAX_LLM_RETRIES + 1} attempts.")
                # å¦‚æœ final_answer ä»ç„¶æ˜¯ None (å³æ‰€æœ‰å˜—è©¦éƒ½å¤±æ•—)ï¼Œå‰‡è¿”å›éŒ¯èª¤
                if final_answer is None:
                    return {"error": f"LLM å›æ‡‰éŒ¯èª¤æˆ–è™•ç†å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦æˆ–æ›´æ›æ¨¡å‹ã€‚"}
                # å¦‚æœä¹‹å‰çš„å˜—è©¦æœ‰çµæœ (é›–ç„¶æœ€å¾Œä¸€æ¬¡å¤±æ•—äº†)ï¼Œå¯ä»¥é¸æ“‡è¿”å›é‚£å€‹çµæœ
                # ä½†é€™è£¡æˆ‘å€‘é‚„æ˜¯ä»¥æœ€å¾Œä¸€æ¬¡å¤±æ•—ç‚ºæº–ï¼Œè¿”å›éŒ¯èª¤
                # return {"error": f"LLM åœ¨æœ€å¾Œä¸€æ¬¡å˜—è©¦ä¸­å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"}


    end_llm_total = time.time()
    logging.info(f"â±ï¸ Total LLM processing time (incl. retries): {end_llm_total - start_llm_total:.2f}s")

    # --- æª¢æŸ¥æœ€çµ‚æ˜¯å¦æœ‰æœ‰æ•ˆç­”æ¡ˆ ---
    if final_answer is None:
        logging.error("âŒ No valid answer generated by LLM after all attempts.")
        return {"error": "ç„¡æ³•å¾èªè¨€æ¨¡å‹ç²å–æœ‰æ•ˆå›æ‡‰"}

    # --- History Update ---
    history.append((question, final_answer))
    # chat_memory[session_id] = history # history å·²ç¶“æ˜¯ chat_memory[session_id] çš„å¼•ç”¨ï¼Œä¸éœ€è¦é‡æ–°è³¦å€¼
    logging.info(f"Session {session_id} history updated.")

    # --- QA Logging ---
    if SAVE_QA:
        try:
            today_str = datetime.now().strftime("%Y-%m-%d")
            qa_filename = QA_LOG_PATH / f"qa_{today_str}.jsonl" # æ”¹ç”¨ .jsonl æé«˜æ•ˆç‡
            qa_record = {
                "session_id": session_id,
                "timestamp": datetime.now().isoformat(),
                "model": selected_model,
                "prompt_mode": prompt_mode,
                "format_mode": format_mode, # è¨˜éŒ„å¯¦éš›ä½¿ç”¨çš„ format_mode
                "question": question,
                "answer": final_answer,
                "llm_attempts": llm_attempts + 1, # è¨˜éŒ„ç¸½å˜—è©¦æ¬¡æ•¸
                "template_used": template_name,
                "retrieved_docs_count": len(docs) # è¨˜éŒ„æª¢ç´¢åˆ°çš„æ–‡æª”æ•¸
            }
            # ä½¿ç”¨ 'a' æ¨¡å¼è¿½åŠ ï¼Œé¿å…è®€å–æ•´å€‹æ–‡ä»¶
            with open(qa_filename, "a", encoding="utf-8") as f:
                f.write(json.dumps(qa_record, ensure_ascii=False) + "\n")
            logging.info(f"ğŸ“ QA record appended to {qa_filename}")
        except Exception as e:
            logging.error(f"âŒ Failed to save QA record: {str(e)}", exc_info=True)

    # --- Return Response ---
    total_time = time.time() - start_all
    logging.info(f"â±ï¸ Total request processing time: {total_time:.2f}s")

    return {
        "answer": final_answer, # è¿”å›æœ€çµ‚è™•ç†å¾Œçš„ answer
        "model_used": selected_model,
        "prompt_mode_used": prompt_mode,
        "format_mode_used": format_mode,
        "template_style_used": template_name
    }


@app.post("/feedback")
async def submit_feedback(feedback: FeedbackRequest):
    """å„²å­˜ä½¿ç”¨è€…å›é¥‹"""
    timestamp = datetime.now().isoformat()
    # ç¢ºä¿æ ¸å¿ƒæ¬„ä½å­˜åœ¨
    if not feedback.user_expected_answer:
        logging.warning("âš ï¸ Feedback submission missing expected answer.")
        from fastapi import HTTPException
        raise HTTPException(status_code=400, detail="é æœŸæ­£ç¢ºå›ç­”ä¸èƒ½ç‚ºç©º")

    record = {
        # ä½¿ç”¨è€…æä¾›çš„ç†æƒ³å•ç­”å°
        "question": feedback.user_expected_question or feedback.question, # å¦‚æœæ²’æä¾›é æœŸå•é¡Œï¼Œä½¿ç”¨åŸå§‹å•é¡Œ
        "answer": feedback.user_expected_answer,
        # åŸå§‹äº’å‹•çš„å…ƒæ•¸æ“š
        "metadata": {
            "source": "manual_feedback",
            "original_question": feedback.question,
            "original_answer": feedback.original_answer,
            "session_id": feedback.session_id,
            "model_used": feedback.model,
            "timestamp": timestamp
        }
    }
    try:
        # ä½¿ç”¨æ›´å…·æè¿°æ€§çš„æ–‡ä»¶å
        ts_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = FEEDBACK_SAVE_PATH / f"feedback_{feedback.session_id}_{ts_str}.json"
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(record, f, ensure_ascii=False, indent=2)
        logging.info(f"ğŸ“ Feedback saved successfully to {filename}")
        return {"message": "âœ… ä½¿ç”¨è€…å›é¥‹å·²å„²å­˜", "filename": str(filename)}
    except Exception as e:
        logging.error(f"âŒ Saving feedback failed: {str(e)}", exc_info=True)
        from fastapi.responses import JSONResponse
        # è¿”å› 500 Internal Server Error
        return JSONResponse(
            status_code=500,
            content={"error": f"å„²å­˜å›é¥‹å¤±æ•—ï¼Œè«‹è¯ç¹«ç®¡ç†å“¡ã€‚"}
        )




# Command to run using uvicorn (recommended for production):
# uvicorn hugging5_1:app --host 0.0.0.0 --port 8000 --workers 1
# ä¸»è¦æ¨¡å‹5/5
