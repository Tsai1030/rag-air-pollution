import os
import json
import logging
import shutil
import fitz  # PyMuPDF
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

# åµŒå…¥èˆ‡å‘é‡è³‡æ–™åº«å¥—ä»¶ï¼ˆæ–°ç‰ˆæœ¬ï¼‰
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

# ---------- Logging è¨­å®š ----------
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ---------- å»ºç«‹å‘é‡è³‡æ–™åº«ï¼ˆæ–°ç‰ˆè‡ªå‹•æŒä¹…åŒ–ï¼‰ ----------
def build_new_vectordb(documents, embeddings, persist_dir="5_5test"): #4_23test
    if os.path.exists(persist_dir):
        logging.info(f"ğŸ§¹ æ¸…é™¤åŸæœ‰è³‡æ–™å¤¾ï¼š{persist_dir}")
        shutil.rmtree(persist_dir, ignore_errors=True)

    vectordb = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory=persist_dir
    )

    logging.info(f"âœ… å‘é‡è³‡æ–™åº«å·²å»ºç«‹ä¸¦è‡ªå‹•å„²å­˜è‡³ï¼š{persist_dir}")
    return vectordb

# ---------- è¼‰å…¥ JSON æ–‡ä»¶ ----------
def load_documents_from_json_files():
    file_paths = [
        r"C:\Users\USER\Desktop\è³‡æ–™åº«ç›®å‰è³‡æ–™\ç¬¬ä¸€æ¬¡æ¸¬è©¦è³‡æ–™\æ–‡ç»è³‡æ–™ä¿®æ­£ç‰ˆ.json",
        r"C:\Users\USER\Desktop\è³‡æ–™åº«ç›®å‰è³‡æ–™\ç¬¬ä¸€æ¬¡æ¸¬è©¦è³‡æ–™\ä¿®æ­£å¾Œçš„ç©ºæ±™qa.json",
        r"C:\Users\USER\Desktop\è³‡æ–™åº«ç›®å‰è³‡æ–™\ç¬¬ä¸€æ¬¡æ¸¬è©¦è³‡æ–™\æ–°èè³‡æ–™ä¿®æ­£ç‰ˆä¸‰å®¶åˆä½µ.json",
        #r"C:\Users\USER\Desktop\è³‡æ–™åº«ç›®å‰è³‡æ–™\ç¬¬ä¸€æ¬¡æ¸¬è©¦è³‡æ–™\å…§éƒ¨æ–‡ä»¶æ¸¬è©¦æª”å«metadata.json",
        r"C:\Users\USER\Desktop\è³‡æ–™åº«ç›®å‰è³‡æ–™\ç¬¬ä¸€æ¬¡æ¸¬è©¦è³‡æ–™\å°šæœªåŠ å…¥4_19\gov_policy_2024_2025.json",
        r"C:\Users\USER\Desktop\è³‡æ–™åº«ç›®å‰è³‡æ–™\ç¬¬ä¸€æ¬¡æ¸¬è©¦è³‡æ–™\å°šæœªåŠ å…¥4_19\æ”¿åºœè³‡æ–™QA.json",
        r"C:\Users\USER\Desktop\è³‡æ–™åº«ç›®å‰è³‡æ–™\ç¬¬ä¸€æ¬¡æ¸¬è©¦è³‡æ–™\å°šæœªåŠ å…¥4_19\ç’°å¢ƒéƒ¨QA.json",
        r"C:\Users\USER\Desktop\è³‡æ–™åº«ç›®å‰è³‡æ–™\ç¬¬ä¸€æ¬¡æ¸¬è©¦è³‡æ–™\å°šæœªåŠ å…¥4_19\air_pollution_enforcement_2025.json",
        r"C:\Users\USER\Desktop\è³‡æ–™åº«ç›®å‰è³‡æ–™\ç¬¬ä¸€æ¬¡æ¸¬è©¦è³‡æ–™\å°šæœªåŠ å…¥4_19\air_pollution_enforcement_2025QA.json",
        r"C:\Users\USER\Desktop\è³‡æ–™åº«ç›®å‰è³‡æ–™\ç¬¬ä¸€æ¬¡æ¸¬è©¦è³‡æ–™\å°šæœªåŠ å…¥4_19\air_quality_monitoring_2023.json",
        # 4/27æ–°å¢
        r"C:\Users\USER\Desktop\è³‡æ–™åº«ç›®å‰è³‡æ–™\æ¸…ç†å¾Œ_å…§éƒ¨æ–‡ä»¶æ¸¬è©¦æª”.json",
        r"C:\Users\USER\Desktop\æ¸…ç†å¾Œç¢ºå®šåŠ å…¥è³‡æ–™\air_quality_report_110_clean.json",
        r"C:\Users\USER\Desktop\æ¸…ç†å¾Œç¢ºå®šåŠ å…¥è³‡æ–™\air_quality_report_111_clean.json",
        r"C:\Users\USER\Desktop\æ¸…ç†å¾Œç¢ºå®šåŠ å…¥è³‡æ–™\air_quality_report_112_clean.json",
        r"C:\Users\USER\Desktop\æ¸…ç†å¾Œç¢ºå®šåŠ å…¥è³‡æ–™\air_quality_report_113_clean.json",
        r"C:\Users\USER\Desktop\æ¸…ç†å¾Œç¢ºå®šåŠ å…¥è³‡æ–™\gptæ•´ç†æˆçš„usrè¨ˆç•«.json",
        r"C:\Users\USER\Desktop\æ¸…ç†å¾Œç¢ºå®šåŠ å…¥è³‡æ–™\grokæ•´ç†usrè¨ˆç•«.json",
        r"C:\Users\USER\Desktop\æ¸…ç†å¾Œç¢ºå®šåŠ å…¥è³‡æ–™\2ã€3ã€ä¸€è‡³ä¸‰.json",
        r"C:\Users\USER\Desktop\æ¸…ç†è™•ç†å¾Œçš„qa\gptæ•´ç†qa_with_docref.json",
        r"C:\Users\USER\Desktop\æ¸…ç†è™•ç†å¾Œçš„qa\GROKæ•´ç†qa_with_docref.json",
        r"C:\Users\USER\Desktop\æ¸…ç†è™•ç†å¾Œçš„qa\2-4qa_gemini_with_docref.json",
        r"C:\Users\USER\Desktop\æ¸…ç†è™•ç†å¾Œçš„qa\110å¹´qa_with_doc_ref.json",
        r"C:\Users\USER\Desktop\æ¸…ç†è™•ç†å¾Œçš„qa\111å¹´qa_with_docref.json",
        r"C:\Users\USER\Desktop\æ¸…ç†è™•ç†å¾Œçš„qa\112å¹´qa_with_docref.json",
        r"C:\Users\USER\Desktop\æ¸…ç†è™•ç†å¾Œçš„qa\113qa_with_docref.json",
        #4/29æ–°å¢
        r"C:\Users\USER\Desktop\429æ¸…ç†\113-116QA.json",
        r"C:\Users\USER\Desktop\429æ¸…ç†\odt_all.json",
        r"C:\Users\USER\Desktop\429æ¸…ç†\ODT_QA_fixed.json",
        r"C:\Users\USER\Desktop\429æ¸…ç†\pollution-control.json",
        r"C:\Users\USER\Desktop\429æ¸…ç†\pollution-control_QA.JSON",
        r"C:\Users\USER\Desktop\429æ¸…ç†\113-116_2_metadata_completed.json",
        r"C:\Users\USER\Desktop\504é«˜é›„fqa\kaohsiung_air_QA.json"
    ]

    long_text_documents = []
    qa_documents = []

    for file_path in file_paths:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            logging.info(f"ğŸ“¥ å·²æˆåŠŸè®€å–ï¼š{os.path.basename(file_path)}")

            for entry in data:
                content = entry.get("page_content") or entry.get("content") or entry.get("text") or ""
                if not content.strip():
                    continue
                metadata = entry.get("metadata", {})
                metadata["source"] = metadata.get("source", os.path.basename(file_path))
                metadata = {
                    k: (v if isinstance(v, (str, int, float, bool)) else str(v))
                    for k, v in metadata.items()
                }

                doc = Document(page_content=f"passage: {content.strip()}", metadata=metadata)

                if any(key in file_path.lower() for key in ["qa", "å•ç­”"]):
                    qa_documents.append(doc)
                else:
                    long_text_documents.append(doc)

        except Exception as e:
            logging.error(f"âŒ {os.path.basename(file_path)} è®€å–å¤±æ•—ï¼š" + str(e))

    if long_text_documents:
        splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)
        long_text_documents = splitter.split_documents(long_text_documents)
        logging.info(f"ğŸ“š é•·æ–‡ chunking å®Œæˆï¼Œå…± {len(long_text_documents)} æ®µ")

    all_docs = qa_documents + long_text_documents
    logging.info(f"ğŸ“¦ å…¨éƒ¨ JSON æ–‡ä»¶åˆä½µå®Œæˆï¼Œå…± {len(all_docs)} ç­†")
    return all_docs

# ---------- è¼‰å…¥ PDF æ–‡ä»¶ ----------
def load_documents_from_pdfs(pdf_paths):
    all_documents = []
    for path in pdf_paths:
        try:
            doc = fitz.open(path)
            for page_num in range(len(doc)):
                page = doc[page_num]
                text = page.get_text().strip()
                if not text:
                    continue
                metadata = {
                    "source": os.path.basename(path),
                    "page": page_num + 1,
                    "doc_id": os.path.splitext(os.path.basename(path))[0]
                }
                all_documents.append(Document(page_content=f"passage: {text}", metadata=metadata))
            logging.info(f"ğŸ“„ å·²è™•ç† PDFï¼š{os.path.basename(path)}ï¼Œå…± {len(doc)} é ")
        except Exception as e:
            logging.error(f"âŒ è®€å– PDF å¤±æ•—ï¼š{os.path.basename(path)} - {e}")
    return all_documents

# ---------- ä¸»ç¨‹å¼ ----------
if __name__ == "__main__":
    model_name = "BAAI/bge-m3"
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs={"device": "cuda"},
        encode_kwargs={"normalize_embeddings": True}
    )
    logging.info(f"âœ… åµŒå…¥æ¨¡å‹å·²è¼‰å…¥ï¼š{model_name}")

    # è¼‰å…¥ JSON èˆ‡ PDF æ–‡ä»¶
    json_documents = load_documents_from_json_files()
    pdf_paths = [
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\æ‰¹é‡è™•ç†\Long-term exposure to NO2 and O3 and all-cause and respiratory mortality A systematic review and meta-analysis.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\æ‰¹é‡è™•ç†\MegaSense_Cyber-PhysicalSystemforReal-timeUrbanAirQualityMonitoring.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\æ‰¹é‡è™•ç†\Optimized machine learning model.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\æ‰¹é‡è™•ç†\Predicting Concentration Levels of Air Pollutants by Transfer.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\æ‰¹é‡è™•ç†\Quantifying the potential effects of air pollution reduction on population health and health expenditure in Taiwan.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\æ‰¹é‡è™•ç†\The association between airborne particulate matter (PM2.5) exposure level.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\æ‰¹é‡è™•ç†\The last decade of air pollution epidemiology and the challenges of quantitative risk assessment.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\æ‰¹é‡è™•ç†\Air Quality Prediction with Physics-Guided Dual Neural ODEs in Open Systems.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\æ‰¹é‡è™•ç†\AirCast Improving Air Pollution Forecasting Through Multi-Variable Data Alignment.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\æ‰¹é‡è™•ç†\AirRadar Inferring Nationwide Air Quality in China with Deep Neural Networks.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\æ‰¹é‡è™•ç†\Ambient air pollution and cardiovascular diseases An umbrella review of systematic reviews and meta-analyses.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\æ‰¹é‡è™•ç†\Association between exposure to air pollution and increased ischaemic stroke incidence a retrospective population-based cohort study (EP-PARTICLES study).pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\æ‰¹é‡è™•ç†\Forecasting Air Quality in Taiwan by Using Machine Learning.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\æ‰¹é‡è™•ç†\Forecasting Smog Clouds With Deep Learning A Proof-Of-Concept.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\æ‰¹é‡è™•ç†\Global, national, and urban burdens of paediatric asthma.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\æ‰¹é‡è™•ç†\Impact of long-term exposure to ambient air pollution on the incidence of chronic obstructive pulmonary disease A systematic review and meta-analysis.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\æ‰¹é‡è™•ç†\Long-term air pollution exposure and incident physical disability.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\æ‰¹é‡è™•ç†\Long-term evaluation of a low-cost air sensor network for monitoring.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\Air Pollution Control Act Enforcement Rules.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\å¤§è³‡æ–™\Guide on Ambient Air Quality Legislation - Air Pollution Series.pdf",
        r"C:\Users\USER\Desktop\æš«å®šå¤–éƒ¨è³‡æ–™åº«æ–‡ç»\å¤§è³‡æ–™\WHO global.pdf"
    ]
    pdf_documents = load_documents_from_pdfs(pdf_paths)

    all_documents = json_documents + pdf_documents
    logging.info(f"ğŸ§© ç¸½æ–‡ä»¶æ•¸é‡ï¼š{len(all_documents)}")

    vectordb = build_new_vectordb(all_documents, embeddings, persist_dir="5_5test") #4_23test #4_27test#4_28test
    logging.info("ğŸ‰ å‘é‡è³‡æ–™åº«å»ºç½®å®Œæˆï¼")
